{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "nameOfTag = newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for text in newsgroups_train.data:\n",
    "    temp_text = nltk.word_tokenize(text)\n",
    "    text = \" \".join([lemmatizer.lemmatize(w) for w in temp_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4500\n",
    "n_components = 20\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                    lowercase=True, stop_words=_stop_words.ENGLISH_STOP_WORDS,\n",
    "                    analyzer='word', binary=True,\n",
    "                    max_df=0.95, min_df=2,\n",
    "                    max_features=n_features\n",
    ")\n",
    "# одновременно создали словарь и преобразовали строку в вектор\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLDA:\n",
    "    def __init__(self, n_components=10, alpha=None, beta=None, max_iter=10):\n",
    "        self._n_components = n_components\n",
    "        self._max_iter = max_iter\n",
    "        self._alpha = alpha\n",
    "        self._beta = beta\n",
    "        self._n_k = None\n",
    "        self._n_k_w = None\n",
    "        self._n_d_k = None\n",
    "\n",
    "        self._fit_Is = False\n",
    "\n",
    "    def fit(self, main_matrix):\n",
    "        self._n_k = np.zeros(self._n_components)                              # колво слов в теге k по всем документам\n",
    "        self._n_k_w = np.zeros((self._n_components, main_matrix.shape[1]))          # колво раз сколько слово w было в теге k\n",
    "        self._n_d_k = np.zeros((main_matrix.shape[0], self._n_components))        # количество вхождений тега k в документе d\n",
    "\n",
    "        if self._alpha == None:\n",
    "            self._alpha = np.ones(self._n_components)\n",
    "        if self._beta == None:\n",
    "            self._beta = np.ones(main_matrix.shape[1])\n",
    "\n",
    "        documn_, word_ =  main_matrix.nonzero()\n",
    "        z = np.random.choice(self._n_components, len(documn_))\n",
    "\n",
    "        for i,j,k in zip(documn_, word_, z):\n",
    "            self._n_k[k] += 1\n",
    "            self._n_k_w[k, j] += 1\n",
    "            self._n_d_k[i, k] += 1\n",
    "        \n",
    "        for i in tqdm(range(self._max_iter)):\n",
    "            for j in range(len(documn_)):\n",
    "                current_word = word_[j]\n",
    "                current_dc = documn_[j]\n",
    "                current_tag = z[j]\n",
    "                self._n_d_k[current_dc, current_tag] -= 1\n",
    "                self._n_k_w[current_tag, current_word] -= 1\n",
    "                self._n_k[current_tag] -= 1\n",
    "                p = (self._n_d_k[current_dc, :] + self._alpha) * (self._n_k_w[:, current_word] + self._beta[current_word]) / (self._n_k + self._beta.sum())\n",
    "                z[j] = np.random.choice(self._n_components, p = p / p.sum())\n",
    "                self._n_d_k[current_dc, z[j]] += 1\n",
    "                self._n_k_w[z[j], current_word] += 1\n",
    "                self._n_k[z[j]] += 1\n",
    "        \n",
    "        self._fit_Is = True\n",
    "        return self\n",
    "    \n",
    "    def get_table_tags_and_word(self):\n",
    "        if self._fit_Is:\n",
    "            return self._n_k_w\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [25:19<00:00, 30.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.customLDA at 0x7ff167b12b20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = customLDA(n_components, max_iter=50)\n",
    "lda.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 1 \tcase\tgovernment\tgun\tguns\tlaw\tlaws\tpeople\tright\trights\tstate\n",
      "Tag 2 \tbike\tbuy\tcar\tcars\tcondition\tgood\tnew\tpower\tsell\tused\n",
      "Tag 3 \tdon\tjust\tknow\tlet\tlike\tpeople\tright\tthink\ttime\twant\n",
      "Tag 4 \tcode\tfile\tfiles\tftp\tprogram\tuse\tusing\tversion\twindow\twindows\n",
      "Tag 5 \tbad\tbetter\tdon\tgood\tjust\tlike\tmake\tright\tthink\tve\n",
      "Tag 6 \tdoes\tdon\tjust\tlike\tproblem\tproblems\ttry\tuse\tusing\twork\n",
      "Tag 7 \tcountry\tgovernment\thistory\tisrael\tisraeli\tjews\tkilled\tpeople\twar\tworld\n",
      "Tag 8 \t1993\tca\tcom\tcontact\tdate\tedu\tinformation\tinternet\tsend\tuniversity\n",
      "Tag 9 \tcard\tcomputer\tdisk\tdrive\tmemory\tpc\tsoftware\tuse\tvideo\twindows\n",
      "Tag 10 \tbelieve\tdid\tdoesn\tgoing\tisn\tjust\tlet\tpeople\tsay\tthings\n",
      "Tag 11 \tcost\tdata\thigh\tlarge\tlong\tlow\tnew\tprogram\tproject\tspace\n",
      "Tag 12 \tago\tgood\tjust\tlike\tnew\tsure\tthink\ttime\tve\tyears\n",
      "Tag 13 \tcame\tday\tdays\tdid\tleft\tnight\tsaw\tsecond\ttook\twent\n",
      "Tag 14 \tbetter\tdoing\tdon\tidea\tlittle\tprobably\treason\tthings\tthink\tway\n",
      "Tag 15 \tbelieve\tbible\tchristian\tdoes\tgod\tjesus\tlife\tpeople\tsay\ttrue\n",
      "Tag 16 \tcause\tcourse\tdon\tedu\tgood\tknow\tlike\tprobably\tsoon\ttime\n",
      "Tag 17 \t10\t11\t12\t15\t17\tgame\tgames\tplay\tteam\tyear\n",
      "Tag 18 \tchip\tclipper\tdata\tencryption\tgovernment\tkey\tkeys\tpublic\tuse\tused\n",
      "Tag 19 \tcom\tedu\temail\tinterested\tknow\tmail\tpost\tquestion\tread\tthanks\n",
      "Tag 20 \tdoes\tdon\thelp\tjust\tknow\tlike\tlooking\tneed\tthanks\twant\n"
     ]
    }
   ],
   "source": [
    "result = np.argsort(lda.get_table_tags_and_word(), axis=1)[:, -n_top_words:]\n",
    "\n",
    "for i in range(n_components):\n",
    "    matrix = np.zeros((1, X_train.shape[1]))\n",
    "    for j in result[i]:\n",
    "        matrix[0, j] = 1\n",
    "    print('Tag {} \\t{}'.format(i + 1, '\\t'.join(vectorizer.inverse_transform(matrix)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сопопставим полученные топ слова для каждого тега:  \n",
    "1. 'talk.politics.mideast'\n",
    "4. 'comp.os.ms-windows.misc'  \n",
    "7. 'talk.politics.guns'  \n",
    "9. 'comp.windows.x'  \n",
    "15. 'soc.religion.christian'  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
