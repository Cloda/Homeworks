{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqUkFb7SAgdF"
      },
      "source": [
        "# HOMEWORK FOR QSTFIN\n",
        "#### Author: Ivan Cherepakhin\n",
        "#### email: icherepaxin@bk.ru\n",
        "#### tg: @User1Usr\n",
        "Дата получения задания: 04.11 \\\n",
        "Дата сдачи задачния: 07.11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7okyCrSAgdH"
      },
      "source": [
        "## 1. MAKE STATEMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oj0tswXAgdI"
      },
      "source": [
        "Надо посторить файл problem_test_labels.csv, в котором для каждого объекта из test написана вероятность принадлежности к категории labels. Для проверки качества результат используем метрику LogLoss(по каждой категории считаем отдельно и потом берется среднее от них)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgkX0KwUAgdI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSlZGY64AgdJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfCk9dSBAgdK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxkEfsDdAgdK"
      },
      "source": [
        "Рассмотрим количество пропусков в labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EqdwscTAgdK"
      },
      "outputs": [],
      "source": [
        "df_labels = pd.read_csv(\"problem_labels.csv\")\n",
        "df_labels.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MK3X2STAgdL"
      },
      "source": [
        "Из условия и этих меток ясно, что задача является multilabel classification. Для ее решения есть множество различных моделей, которые мы проверим далее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GtJnlQkAgdM"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"problem_train.csv\")\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw-WGmFIAgdM"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(\"problem_test.csv\")\n",
        "df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PksY4D8rAgdM"
      },
      "source": [
        "## 2. DATA TRANSFORM AND ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0jw1ITKAgdN"
      },
      "source": [
        "### 2.1. Work with NaN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_CNqys6AgdN"
      },
      "source": [
        "Поработаем с пропусками."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoMl500yAgdN"
      },
      "outputs": [],
      "source": [
        "df_train.replace('NaN', np.nan, inplace=True) # замена строк NaN на настоящие\n",
        "print(\"Полностью заполненных столбцов = \", df_train.notna().all().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud1SPoUTAgdN"
      },
      "outputs": [],
      "source": [
        "print(\"Есть пустые слобцы = \", df_train.shape[1] != df_train.isnull().all().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXMAjSzYAgdO"
      },
      "source": [
        "Убирем эти столбцы из train и test, тк они не помогут в обучении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhQcH4JVAgdO"
      },
      "outputs": [],
      "source": [
        "names_col_filtr = df_train.isnull().all()\n",
        "all_nan_col = names_col_filtr.index * names_col_filtr.values\n",
        "all_nan_col = all_nan_col.drop_duplicates()[1:]\n",
        "df_train = df_train.drop(columns=all_nan_col)\n",
        "df_test = df_test.drop(columns=all_nan_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or-CVXtuAgdO"
      },
      "source": [
        "Поработаем сначала с полностью очищенной от NaN датой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ3wUlwXAgdO"
      },
      "outputs": [],
      "source": [
        "full_nonNan_df_train = df_train.dropna(axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvDpvNMZAgdP"
      },
      "source": [
        "Заметим, что в test те же столбцы, кроме одного, являются полностью заполнеными. Это говорит нам о некоторой закономерности заполнения данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7mkrt3PAgdP"
      },
      "outputs": [],
      "source": [
        "full_nonNan_df_test = df_test[full_nonNan_df_train.columns]\n",
        "check_to_nan = full_nonNan_df_test.notna().all()\n",
        "print(\"Столбцов, которые полностью заполнены = {}(из {})\".format(full_nonNan_df_test.notna().all().sum(), full_nonNan_df_test.shape[1]))\n",
        "col_of_nan = check_to_nan[check_to_nan == False].index\n",
        "col_of_nan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ewnu4NjAgdP"
      },
      "source": [
        "Заметим, что отсутсвующее значение только в одной строке. Возьмем id этой стороки(может пригодиться для дальнейшего изучения)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiTgLgMMAgdP"
      },
      "outputs": [],
      "source": [
        "index_of_nan = full_nonNan_df_test['c_1348'][full_nonNan_df_test['c_1348'].isna() == True].index\n",
        "full_nonNan_df_test = full_nonNan_df_test.dropna()\n",
        "index_of_nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hYew-CKAgdQ"
      },
      "source": [
        "### 2.2. Work with dublicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb2i3ekeAgdQ"
      },
      "source": [
        "Теперь проверим на наличие дубликатов id в test и train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0dbo9j1AgdQ"
      },
      "outputs": [],
      "source": [
        "set(full_nonNan_df_train.id) & set(full_nonNan_df_test.id) == set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n_9sO2QAgdQ"
      },
      "source": [
        "Проверка на совпадение типов в test и train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPXAkJ_5AgdR"
      },
      "outputs": [],
      "source": [
        "sum(full_nonNan_df_train.dtypes == full_nonNan_df_test.dtypes) == len(full_nonNan_df_train.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNs-jP7hAgdR"
      },
      "source": [
        "### 2.3. Work with category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t77kCAEAgdR"
      },
      "source": [
        "Разберемся, что делать с нечисловыми переменными. Сложно понять какие фичи являются просто переменными, а какие конкретными значениями, для каких существенным является порядок, а для каких нет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhQysiQ6AgdR"
      },
      "outputs": [],
      "source": [
        "print(\"Нечисловых столбцов = \", sum(full_nonNan_df_train.dtypes == \"object\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTuQL_5SAgdS"
      },
      "source": [
        "На первый взгяд кажется, что в числовых фичах много дубликатов. Избавимся от них, дабы избежать overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKG7olUOAgdS"
      },
      "outputs": [],
      "source": [
        "full_nonNan_df_train = full_nonNan_df_train.T.drop_duplicates().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2TYqS6kAgdS"
      },
      "outputs": [],
      "source": [
        "full_nonNan_df_test = full_nonNan_df_test[full_nonNan_df_train.columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1AYW3wWAgdS"
      },
      "source": [
        "Рассмотрим столбец release. Можно предположить, что этот слолбец все таки является категориальным, тк мы нам дано название столбца и в нем всего 3 состояния. Применим one-hot encoder для преобразования в числовой вариант."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH01qa5RAgdS"
      },
      "outputs": [],
      "source": [
        "full_nonNan_df_train = pd.get_dummies(full_nonNan_df_train, columns=[\"release\"])\n",
        "full_nonNan_df_test = pd.get_dummies(full_nonNan_df_test, columns=[\"release\"])\n",
        "full_nonNan_df_train = full_nonNan_df_train.apply(pd.to_numeric, errors='ignore')\n",
        "full_nonNan_df_test = full_nonNan_df_test.apply(pd.to_numeric, errors='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V68Lxv0AgdS"
      },
      "source": [
        "Рассмотрим количество уникальных значений в категориальных столбцах, чтобы оценить, насколько увеличится датасет при one-hot и чтобы оптимально и без потерь преобразовать категории."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlqdiEbFAgdT"
      },
      "outputs": [],
      "source": [
        "list_of_category_variable = full_nonNan_df_train.dtypes.loc[full_nonNan_df_train.dtypes == \"object\"].index\n",
        "list_of_num_variable = full_nonNan_df_train.dtypes.loc[full_nonNan_df_train.dtypes != \"object\"].index\n",
        "unique_elem_in_col = {}\n",
        "\n",
        "for col in list_of_category_variable:\n",
        "    unique_elem_in_col[col] = list(zip(full_nonNan_df_train[col].value_counts().to_dict().keys(), full_nonNan_df_train[col].value_counts().to_dict().values()))\n",
        "\n",
        "unique_elem_in_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_15_7pUAgdm"
      },
      "outputs": [],
      "source": [
        "list_of_category_variable = full_nonNan_df_test.dtypes.loc[full_nonNan_df_test.dtypes == \"object\"].index\n",
        "list_of_num_variable = full_nonNan_df_test.dtypes.loc[full_nonNan_df_test.dtypes != \"object\"].index\n",
        "unique_elem_in_col = {}\n",
        "\n",
        "for col in list_of_category_variable:\n",
        "    unique_elem_in_col[col] = list(zip(full_nonNan_df_test[col].value_counts().to_dict().keys(), full_nonNan_df_test[col].value_counts().to_dict().values()))\n",
        "\n",
        "unique_elem_in_col"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7YjIiJaAgdn"
      },
      "source": [
        "Заметим, что c_1158 и c_1348 в train содержат такие категории у которых маленькая частота. К сожалению, какого то четкого порядка и зависимостей пока не удается заметить."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltcnO-LjAgdn"
      },
      "outputs": [],
      "source": [
        "onehotencoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYNRv5JbAgdn"
      },
      "outputs": [],
      "source": [
        "data_new = onehotencoder.fit_transform(full_nonNan_df_train[list_of_category_variable])\n",
        "full_nonNan_df_train = pd.concat([full_nonNan_df_train,data_new], axis=1).drop(columns=list_of_category_variable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfPOo6D2Agdn"
      },
      "outputs": [],
      "source": [
        "data_new = onehotencoder.transform(full_nonNan_df_test[list_of_category_variable])\n",
        "full_nonNan_df_test = pd.concat([full_nonNan_df_test, data_new], axis=1).drop(columns=list_of_category_variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_LPfXMHAgdn"
      },
      "source": [
        "### 2.4 Ploting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdvU8pxAgdn"
      },
      "source": [
        "Отрисуем числовые фичи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I__ynFXqAgdo"
      },
      "outputs": [],
      "source": [
        "full_nonNan_df_train[list_of_num_variable].drop(columns=[\"id\", \"release_a\", \"release_b\", \"release_c\", \"n_0047\"]).hist(figsize=(18,12), bins=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SU_EUCBAgdo"
      },
      "source": [
        "Говорить о каком то распределние трудно, но можно заметить, что n_0078 и n_0108 очень похожи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEcxNFVtAgdo"
      },
      "source": [
        "## 3. DATA SCIENCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCc4-DKUAgdo"
      },
      "source": [
        "Попробуем применять методы машинного обучения. Вначале масштабируем данные."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnXz17AzAgdo"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler().set_output(transform=\"pandas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAx-mPNmAgdp"
      },
      "source": [
        "Не будем масштабировать int фичи, так как могут быть категориями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pokkp6IHAgdp"
      },
      "outputs": [],
      "source": [
        "X = full_nonNan_df_train.copy()\n",
        "X.set_index(\"id\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUtK01DvAgdp"
      },
      "outputs": [],
      "source": [
        "main_test = full_nonNan_df_test.copy()\n",
        "main_test.set_index(\"id\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePbYj-06Agdp"
      },
      "outputs": [],
      "source": [
        "y = pd.read_csv(\"problem_labels.csv\").set_index('id')\n",
        "y_col = y.columns\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjQUa3bPAgdq"
      },
      "source": [
        "Для начальной проверки моделей, разобьем train на обчающую и тестовую часть."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQbUwIxNU0mw"
      },
      "outputs": [],
      "source": [
        "y.columns = np.arange(0, len(y.columns))\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVPhSi0MAgdq"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9lVkT01Agdq"
      },
      "source": [
        "Добавим следующие трансофрмеры для pipline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXSMtUvTFCcB"
      },
      "outputs": [],
      "source": [
        "from numba import njit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IepQV5W9Agdq"
      },
      "outputs": [],
      "source": [
        "slc = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07UDuSOcAgdr"
      },
      "outputs": [],
      "source": [
        "pca = PCA()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io3r4zyjAgdr"
      },
      "outputs": [],
      "source": [
        "def custom_metrics_log_loss_by_col(y_true, y_pred):\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_true = y_true.T\n",
        "    mean_log_loss = 0\n",
        "\n",
        "    # надо оптимизировать\n",
        "    # написать свою векторизированную функцию log_loss и применить njit из numba\n",
        "    for i in range(y_true.shape[1]):\n",
        "        mean_log_loss += log_loss(y_true[:,i], y_pred[:,i,:])\n",
        "\n",
        "    return mean_log_loss / y_true.shape[1]\n",
        "\n",
        "custom_log_loss = make_scorer(custom_metrics_log_loss_by_col, needs_proba=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-ion1fwAgdr"
      },
      "source": [
        "Настроим модель DecisionTreeClassifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9PlLUDxKAgdr"
      },
      "outputs": [],
      "source": [
        "clf = MultiOutputClassifier(DecisionTreeClassifier(random_state=0))\n",
        "\n",
        "pipe_tree = Pipeline(\n",
        "            steps=[\n",
        "                    ('pca', pca),\n",
        "                    ('tree', clf)\n",
        "])\n",
        "\n",
        "\n",
        "n_components = [5, 10]\n",
        "\n",
        "param = [{\n",
        "            'pca__n_components': n_components,\n",
        "            'tree__estimator__max_depth': [6, 12],\n",
        "            'tree__estimator__max_features': [5, 10]\n",
        "}]\n",
        "\n",
        "dtc_model = GridSearchCV(pipe_tree, param, scoring=custom_log_loss, cv=5).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ad_UsRuAgdr",
        "outputId": "7e6392fe-124f-4d9c-b1bb-41f4dddcb0a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.24821543254688"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree_best_estim = dtc_model.best_estimator_\n",
        "y_test_pred = tree_best_estim.predict_proba(X_test)\n",
        "y_test_score = custom_metrics_log_loss_by_col(y_test, y_test_pred)\n",
        "y_test_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KREWHAZwAgds"
      },
      "source": [
        "Настроим модель kNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsSx4l0sAgds"
      },
      "outputs": [],
      "source": [
        "clf = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=1))\n",
        "\n",
        "pipe_knn = Pipeline(\n",
        "            steps=[ ('slc', slc,),\n",
        "                    ('pca', pca),\n",
        "                    ('kNN', clf)\n",
        "])\n",
        "\n",
        "\n",
        "param = [{\n",
        "            'pca__n_components': n_components,\n",
        "            'kNN__estimator__n_neighbors': np.arange(1, 30, 5),\n",
        "}]\n",
        "\n",
        "knn_model = GridSearchCV(pipe_knn, param, scoring=custom_log_loss, cv=5).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpNzXtxIAgds"
      },
      "outputs": [],
      "source": [
        "knn_best_estim = knn_model.best_estimator_\n",
        "y_test_pred = knn_best_estim.predict_proba(X_test)\n",
        "y_test_score = custom_metrics_log_loss_by_col(y_test, y_test_pred)\n",
        "y_test_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcJP-6EpAgdt"
      },
      "source": [
        "Настроим MLPClassifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CV5ZjcdAgdt"
      },
      "outputs": [],
      "source": [
        "clf = MultiOutputClassifier(MLPClassifier(solver='adam',\n",
        "                                            learning_rate_init=0.01,\n",
        "                                            max_iter=300,\n",
        "                                            activation='relu',\n",
        "                                            early_stopping=True))\n",
        "\n",
        "pipe_mlp = Pipeline(\n",
        "            steps=[ ('slc', slc,),\n",
        "                    ('pca', pca),\n",
        "                    ('mlp', clf)\n",
        "])\n",
        "\n",
        "param = [{\n",
        "            'pca__n_components': n_components,\n",
        "}]\n",
        "\n",
        "mpl_model = GridSearchCV(pipe_mlp, param, scoring=custom_log_loss, cv=5).fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P3WR-ZAAgdt"
      },
      "outputs": [],
      "source": [
        "mlp_best_estim = mpl_model.best_estimator_\n",
        "y_test_pred = mlp_best_estim.predict_proba(X_test)\n",
        "y_test_score = custom_metrics_log_loss_by_col(y_test, y_test_pred)\n",
        "y_test_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ykMDVqgpET"
      },
      "source": [
        "Из показателей score на тесте, делаем вывод, что лучше всего обучаться на последней моделе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewoMMNPAelAd"
      },
      "outputs": [],
      "source": [
        "clf = MultiOutputClassifier(MLPClassifier(solver='adam',\n",
        "                                            learning_rate_init=0.01,\n",
        "                                            max_iter=300,\n",
        "                                            activation='relu',\n",
        "                                            early_stopping=True))\n",
        "\n",
        "pipe_mlp = Pipeline(\n",
        "            steps=[ ('slc', slc,),\n",
        "                    ('pca', pca),\n",
        "                    ('mlp', clf)\n",
        "])\n",
        "\n",
        "n_components = [25, 50, 100]\n",
        "\n",
        "param = [{\n",
        "            'pca__n_components': n_components,\n",
        "            'mlp__estimator__alpha': 10.0 ** -np.arange(1, 5),\n",
        "}]\n",
        "\n",
        "mpl_model = GridSearchCV(pipe_mlp, param, scoring=custom_log_loss, cv=5).fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M06KUzpUhcAA"
      },
      "outputs": [],
      "source": [
        "mpl_best = mpl_model.best_estimator_\n",
        "main_test_pred = np.array(mpl_best.predict_proba(main_test))\n",
        "main_test_pred = pd.DataFrame(main_test_pred[:,:,1]).T\n",
        "main_test_pred.index = main_test.index\n",
        "main_test_pred.columns = y_col\n",
        "main_test_pred.to_csv(\"problem_test_labels.csv\", sep=';', encoding='utf-8')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
